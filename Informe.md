# Entrenamiento distribuido de un LLM


Integrantes:
- Kiara Alexandra Balcázar Santa Cruz (100%)
- Arturo Magno Barrantes Chuquimia (100%)
- Andrea Fernanda Coa Cruz (100%)

## Código/PRAM

El código es un fork del repositorio [llm.c](https://github.com/karpathy/llm.c) de [Andrej Karpathy](https://github.com/karpathy), que busca implementar el LLM GPT-2 de forma minimalista. Tiene varias implementaciones. En particular, usamos las implementaciones en CUDA (para entrenamiento distribuido) y en C (para entrenamiento secuencial y en un solo nodo).

